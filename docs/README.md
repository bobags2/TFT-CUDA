# TFT-CUDA: Temporal Fusion Transformer with CUDA Acceleration

A high-performance implementation of the Temporal Fusion Transformer (TFT) optimized for financial time-series forecasting with CUDA acceleration.

## Features

üöÄ **CUDA-Accelerated Operations**

- Custom CUDA kernels for LSTM, Multi-Head Attention, and Quantile heads
- Mixed-precision training (FP16/FP32) for memory efficiency
- Optimized memory management and tensor operations

üìà **Financial ML Focus**  

- Multi-asset time-series modeling (ES, VX, ZN futures)
- Comprehensive feature engineering (30+ financial indicators)
- Microstructure-aware features (order flow, Kyle's lambda, liquidity proxies)
- Regime detection and cross-asset correlation analysis

üîç **Advanced Interpretability**

- Attention weight visualization and temporal importance analysis
- Variable Selection Network (VSN) feature importance
- Counterfactual analysis for model understanding
- Interactive visualizations with Plotly

‚ö° **Production-Ready**

- Quantile loss for probabilistic forecasting
- Multi-horizon prediction capabilities
- Robust training with early stopping and gradient clipping
- Comprehensive logging and checkpointing

## üöÄ Quickstart

### One-Click Setup

```bash
# 1. Build CUDA & install
bash scripts/build.sh

# 2. Train model
bash scripts/train.sh

# 3. Run tests
bash scripts/test.sh

# 4. Debug (if needed)
bash scripts/debug.sh
```

### Even Simpler with Make

```bash
# Build, train, and test everything
make all

# Or run individual steps
make build
make train
make test
make debug
```

### Traditional Installation

```bash
# Clone the repository
git clone https://github.com/bobags2/TFT-CUDA.git
cd TFT-CUDA

# Install dependencies (handled automatically by build.sh)
pip install torch numpy pandas pybind11

# Install package (with CUDA support if available)
pip install -e .
```

### Basic Usage

```python
import tft_cuda
import torch

# 1. Load and process financial data
dataset = tft_cuda.FinancialDataset(data_dir="data/")
processed_data = dataset.process_pipeline()

# 2. Create sequences for training
X, y = dataset.create_sequences(processed_data, sequence_length=100)
X_train, X_val, X_test, y_train, y_val, y_test = dataset.train_val_test_split(X, y)

# 3. Create TFT model
config = tft_cuda.create_tft_config(input_size=X.shape[-1])
model = tft_cuda.TemporalFusionTransformer(config)

# 4. Setup training
training_config = tft_cuda.create_training_config()
trainer = tft_cuda.TFTTrainer(model, training_config)

# 5. Train the model
from torch.utils.data import DataLoader
train_dataset = tft_cuda.TFTDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

history = trainer.train(train_loader, epochs=50)

# 6. Generate predictions
sample_inputs = {
    'historical_features': torch.tensor(X_test[:1]),
    'static_features': torch.zeros(1, 10)
}
predictions = model.predict(sample_inputs)
```

## Data Format

The system expects 10-minute financial data with the following schema:

### Input Files

- `es10m.csv` - E-mini S&P 500 Futures
- `vx10m.csv` - VIX Volatility Index  
- `zn10m.csv` - 10-Year US Treasury Yield

### Column Format

```
Date, Time, Open, High, Low, Last, Volume, NumberOfTrades, BidVolume, AskVolume
```

### Feature Engineering

The system automatically generates 30+ features including:

**Price & Volume Features**

- Log returns at multiple lags
- Price ranges and volatility measures
- Volume z-scores and ratios

**Microstructure Features**

- Order Flow Imbalance (OFI)
- Kyle's Lambda proxy
- Bid-ask spread and skew
- Liquidity depth proxies

‚ö†Ô∏è **CRITICAL: Data Leakage Prevention**

The system includes **temporal-aware feature engineering** to prevent data leakage:

- **Temporal Split FIRST**: Train/val/test split happens BEFORE feature engineering
- **Incremental Features**: All features computed using only past/present data
- **Rolling Statistics**: Computed with expanding windows to avoid future information
- **Production-Ready**: Realistic performance metrics without data snooping

Use `bash scripts/train_temporal.sh` for leakage-free training.

- Bid-ask spread and skew
- Liquidity depth proxies

**Technical Indicators**

- RSI, MACD, Bollinger Bands
- Moving averages and crossovers
- VWAP and ATR

**Cross-Asset Features**

- VIX regime classification
- Cross-asset correlations
- Divergence signals

**Time-Based Features**

- Market session indicators
- Intraday patterns
- Economic calendar events

## Model Architecture

The TFT implementation includes:

- **Variable Selection Networks** for automatic feature selection
- **LSTM encoder** for temporal pattern recognition  
- **Multi-Head Attention** with causal masking for temporal fusion
- **Quantile heads** for probabilistic predictions (0.1, 0.5, 0.9 quantiles)
- **Gated Residual Networks** throughout for information flow

## CUDA Acceleration

When CUDA is available, the following operations are accelerated:

- LSTM forward/backward passes with variable selection
- Multi-Head Attention with RoPE positional encoding
- Linear layer operations with mixed precision
- Quantile loss computation
- Attention aggregation for interpretability

## Configuration

### Model Configuration

```python
config = {
    'input_size': 64,           # Number of input features
    'hidden_size': 256,         # Hidden layer dimensions
    'num_heads': 8,             # Attention heads
    'quantile_levels': [0.1, 0.5, 0.9],  # Prediction quantiles
    'prediction_horizon': [1, 5, 10],     # Steps ahead to predict
    'sequence_length': 100,     # Input sequence length
    'dropout_rate': 0.1         # Dropout for regularization
}
```

### Training Configuration

```python
training_config = {
    'optimizer': 'adamw',
    'learning_rate': 9.38e-06,
    'batch_size': 64,
    'epochs': 100,
    'early_stopping': True,
    'patience': 15,
    'grad_clip_norm': 1.0
}
```

## Performance

Typical performance metrics on financial data:

- **Training Speed**: 2-3x faster with CUDA acceleration
- **Memory Usage**: 30-40% reduction with mixed precision
- **Prediction Accuracy**: Superior to baseline LSTM/GRU models
- **Sharpe Ratio**: Target >1.5 on out-of-sample data

## Requirements

- Python 3.8+
- PyTorch 1.13+
- CUDA 11.0+ (optional, for acceleration)
- NumPy, Pandas, SciPy
- Matplotlib, Seaborn (for visualization)
- Optional: Plotly, Weights & Biases

## Development

### Building from Source

```bash
# Clone repository
git clone https://github.com/bobags2/TFT-CUDA.git
cd TFT-CUDA

# Install development dependencies
pip install -e ".[dev]"

# Run tests
pytest tests/

# Format code
black python/
```

### CUDA Development

To build with CUDA support:

```bash
# Ensure CUDA toolkit is installed
nvcc --version

# Build with CUDA
CMAKE_ARGS="-DCMAKE_CUDA_ARCHITECTURES=75;80;86" pip install -e .
```

## Citation

If you use TFT-CUDA in your research, please cite:

```bibtex
@software{tft_cuda_2024,
  title={TFT-CUDA: High-Performance Temporal Fusion Transformer for Financial Forecasting},
  author={TFT-CUDA Team},
  year={2024},
  url={https://github.com/bobags2/TFT-CUDA}
}
```

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Support

- üìß Email: <contact@tft-cuda.com>
- üêõ Issues: [GitHub Issues](https://github.com/bobags2/TFT-CUDA/issues)
- üìñ Documentation: [ReadTheDocs](https://tft-cuda.readthedocs.io/)
